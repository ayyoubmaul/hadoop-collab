{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "\n",
        "Hadoop is an open-source framework which is mainly used for storage purposes and maintaining and analyzing a large amount of data or datasets on the clusters of commodity hardware, which means it is actually a data management tool.\n",
        "\n",
        "##Hadoop mainly works on 3 different modes:\n",
        "\n",
        "###Standalone Mode\n",
        "\n",
        "Pseudo-distributed Mode\n",
        "Fully-distributed Mode\n",
        "Standalone Mode\n",
        "\n",
        "By default, Hadoop is configured to run in a non distributed mode. It runs as a single Java process. Instead of HDFS, this mode utilizes the local file system. This mode is useful for debugging and there isn't any need to configure core-site.xml, hdfs-site.xml, mapred-site.xml, masters & slaves. Stand-alone mode is usually the fastest mode in Hadoop.\n",
        "\n",
        "###Pseudo-distributed Mode\n",
        "\n",
        "Hadoop can also run on a single node in a Pseudo-distributed mode. In this mode, each daemon runs on separate java processes. In this mode custom configuration is required (core-site.xml, hdfs-site.xml, mapred-site.xml). Here HDFS is utilized for input and output. This mode of deployment is useful for testing and debugging purposes.\n",
        "\n",
        "###Fully-distributed Mode\n",
        "\n",
        "This is the production mode of Hadoop. In this mode typically one machine in the cluster is designated as NameNode and another as Resource Manager exclusively. These are masters. All other nodes act as Data Node and Node Manager. These are the slaves. Configuration parameters and environment need to be specified for Hadoop Daemons.\n",
        "\n",
        "Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ],
      "metadata": {
        "id": "QIsVT46Je21m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Java 8\n",
        "Hadoop is a java programming-based data processing framework\n",
        "\n",
        "OpenJDK is a development environment for building applications, applets, and components using the Java programming language."
      ],
      "metadata": {
        "id": "SypF2Xj6OvmD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lqfPfV3BIxlK"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5reX4ezLYdv",
        "outputId": "48dc0e61-bd34-4d94-b3c8-aae978f90513"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"11.0.22\" 2024-01-16\n",
            "OpenJDK Runtime Environment (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.22+7-post-Ubuntu-0ubuntu222.04.1, mixed mode, sharing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ai6F2rBcPIui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60OG2VvRL91q",
        "outputId": "61b99cfe-a9fa-4e74-9240-7dd6ce841d4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePeQrFRnMpr6",
        "outputId": "8046130e-5982-49a2-ab55-066a12dc7165"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update JPS (Java Process Status)"
      ],
      "metadata": {
        "id": "uAJINg88PcvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DqtyMUGMxW0",
        "outputId": "dadf80ac-bce3-498f-d34d-67c70117e7e2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### See Java version"
      ],
      "metadata": {
        "id": "o8zQfyYLPmO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GAxAwOTM5GE",
        "outputId": "a2da3eff-b174-4102-cd58-e027e6a3a73e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_402\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_402-8u402-ga-2ubuntu1~22.04-b06)\n",
            "OpenJDK 64-Bit Server VM (build 25.402-b06, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Java related environment variables\n",
        "\n",
        "The JAVA_HOME is an operating system environment variable points to the file system location where the JDK or JRE was installed.\n",
        "\n"
      ],
      "metadata": {
        "id": "NDxQNq6tQDJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Hsa44KM8gL",
        "outputId": "50f66550-ba91-483c-bc07-43b059c3bbc4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64/jre/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Secure Shell Server (SSHD)\n",
        "We need to define a means for the master node to remotely access every node in our cluster.\n",
        "\n",
        "Hadoop uses passphrases SSH for the communication between the nodes.\n",
        "\n",
        "SSH is a cryptographic network protocol for operating network services securely over an unsecured network.\n",
        "\n",
        "SSH utilizes standard public key criptography to create a pair of keys for user verification:\n",
        "\n",
        "one public and one private.\n",
        "\n",
        "The **pseudo distributed mode** is special case of fully the distributed mode, in which the single host is localhost (our machine). We need to make sure that to access to localhost and login we do not need to enter a password. Therefore, SSH needs to be set up to allow passwordless login for the Hadoop user. The simplest way to achive this is to generate a public-private key pair."
      ],
      "metadata": {
        "id": "ExVDx5g6QK97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openssh-server -qq > /dev/null\n",
        "!service ssh start\n",
        "\n",
        "!grep Port /etc/ssh/sshd_config\n",
        "\n",
        "#Creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb9oTBwNSbkh",
        "outputId": "8c1f17bb-7ebc-4f90-a91f-4af7c121a752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n",
            "#Port 22\n",
            "#GatewayPorts no\n",
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:jRZrE7qjuXEc96VXEV66eZ4RP+AdzcqrqXDafO0Ldno root@3db6ed18c7d2\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|              . .|\n",
            "|             . =.|\n",
            "|        o    .+oo|\n",
            "|       . *  ..o=+|\n",
            "|      o S . ..*+o|\n",
            "|     . * o o . ++|\n",
            "|    . = . + +.o..|\n",
            "|     = . * o.*E  |\n",
            "|    +.  . +o+oo. |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See id_rsa.pub content\n",
        "!more /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIFO0DqCQrOC",
        "outputId": "e748212b-d106-4fed-d6f7-4cc37c6f01b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDz+fq42UXBEtw/ywFM6jAySSQY8m/oGYRn0q/Rnpr0mrpAsNjtj6e7AkjvrakB\n",
            "bsNiRLBfaOx2pHIkB22Xp2toXodD4Mrk21Yzf7rYlWCPEcgvkF43wmoKlLQ+Pdp4qN1NB1zTvLzBH1utEyIv4W8kIKbT2xwGtUKs\n",
            "UNee5xhsa+2OrWyfU3SiY8F/LOklPNmTApAHiGRUR3i5O7mESEZnC4MvDc+vVRhkJY+AiRkaCP8/9oDKxRsoEFeObewZkkD6swOE\n",
            "cfw9usnwH9+uvT6G+d5KHhrC9RwkIsM9cUcf1LRmcf2273q48Y6smTm4JMsHrSkH2tYk+ZJHGEYa1bCVS5L2FPed6KJz3w/wQAOi\n",
            "k6fQs/1FsDNtE96z6Y1MaYKW4UIJ5h+79jd+vrULYQxd7prhHfqaqMStpeWqVkAFmWkHy6JYCAUt/pUjIjj0t4ddItGNcUVOleqz\n",
            "3WZm575BnXpZzBW3jqzliC68+t8FfVJKVX6xcukCNtJzY3ieFYc= root@3db6ed18c7d2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys\n",
        "#Changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys\n",
        "\n",
        "#Conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4uUcansS2Ao",
        "outputId": "151c94e8-4ad7-41a6-e086-e8bb739d739f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ED25519) to the list of known hosts.\r\n",
            " 17:33:59 up 1 min,  0 users,  load average: 1.45, 0.62, 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "P584CwG-RmpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz\n",
        "\n",
        "#Untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "#Removing the tar file\n",
        "!rm hadoop-3.2.3.tar.gz\n",
        "\n",
        "\n",
        "#Copying the hadoop files to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/\n",
        "#-r copy directories recursively"
      ],
      "metadata": {
        "id": "YDP8d7hgTIq3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop directory\n",
        "!ls /usr/local/hadoop-3.2.3/etc/hadoop\n",
        "#we can see various configuration files of hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AanxcC1zThNK",
        "outputId": "24f22d73-5001-442f-d6a9-09ee9334ece3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "capacity-scheduler.xml\t\t  httpfs-log4j.properties     mapred-site.xml\n",
            "configuration.xsl\t\t  httpfs-signature.secret     shellprofile.d\n",
            "container-executor.cfg\t\t  httpfs-site.xml\t      ssl-client.xml.example\n",
            "core-site.xml\t\t\t  kms-acls.xml\t\t      ssl-server.xml.example\n",
            "hadoop-env.cmd\t\t\t  kms-env.sh\t\t      user_ec_policies.xml.template\n",
            "hadoop-env.sh\t\t\t  kms-log4j.properties\t      workers\n",
            "hadoop-metrics2.properties\t  kms-site.xml\t\t      yarn-env.cmd\n",
            "hadoop-policy.xml\t\t  log4j.properties\t      yarn-env.sh\n",
            "hadoop-user-functions.sh.example  mapred-env.cmd\t      yarnservice-log4j.properties\n",
            "hdfs-site.xml\t\t\t  mapred-env.sh\t\t      yarn-site.xml\n",
            "httpfs-env.sh\t\t\t  mapred-queues.xml.template\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to configure a few things before running Hadoop. That is, we need to either add or modify few parameters in these configuration files to operate Hadoop in whichever mode we want to.\n",
        "\n",
        "Configuring hadoop-env.sh file\n",
        "\n",
        "hadoop-env.sh is a bash script that containts environment variables that are used in the scripts to run Hadoop"
      ],
      "metadata": {
        "id": "ycmuckIeR07B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-env.sh file\n",
        "!cat /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCvcoqYaTs80",
        "outputId": "d59af738-d0f6-4ed1-8dc2-265ea2924700"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#\n",
            "# Licensed to the Apache Software Foundation (ASF) under one\n",
            "# or more contributor license agreements.  See the NOTICE file\n",
            "# distributed with this work for additional information\n",
            "# regarding copyright ownership.  The ASF licenses this file\n",
            "# to you under the Apache License, Version 2.0 (the\n",
            "# \"License\"); you may not use this file except in compliance\n",
            "# with the License.  You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\n",
            "# Set Hadoop-specific environment variables here.\n",
            "\n",
            "##\n",
            "## THIS FILE ACTS AS THE MASTER FILE FOR ALL HADOOP PROJECTS.\n",
            "## SETTINGS HERE WILL BE READ BY ALL HADOOP COMMANDS.  THEREFORE,\n",
            "## ONE CAN USE THIS FILE TO SET YARN, HDFS, AND MAPREDUCE\n",
            "## CONFIGURATION OPTIONS INSTEAD OF xxx-env.sh.\n",
            "##\n",
            "## Precedence rules:\n",
            "##\n",
            "## {yarn-env.sh|hdfs-env.sh} > hadoop-env.sh > hard-coded defaults\n",
            "##\n",
            "## {YARN_xyz|HDFS_xyz} > HADOOP_xyz > hard-coded defaults\n",
            "##\n",
            "\n",
            "# Many of the options here are built from the perspective that users\n",
            "# may want to provide OVERWRITING values on the command line.\n",
            "# For example:\n",
            "#\n",
            "#  JAVA_HOME=/usr/java/testing hdfs dfs -ls\n",
            "#\n",
            "# Therefore, the vast majority (BUT NOT ALL!) of these defaults\n",
            "# are configured for substitution and not append.  If append\n",
            "# is preferable, modify this file accordingly.\n",
            "\n",
            "###\n",
            "# Generic settings for HADOOP\n",
            "###\n",
            "\n",
            "# Technically, the only required environment variable is JAVA_HOME.\n",
            "# All others are optional.  However, the defaults are probably not\n",
            "# preferred.  Many sites configure these options outside of Hadoop,\n",
            "# such as in /etc/profile.d\n",
            "\n",
            "# The java implementation to use. By default, this environment\n",
            "# variable is REQUIRED on ALL platforms except OS X!\n",
            "# export JAVA_HOME=\n",
            "\n",
            "# Location of Hadoop.  By default, Hadoop will attempt to determine\n",
            "# this location based upon its execution path.\n",
            "# export HADOOP_HOME=\n",
            "\n",
            "# Location of Hadoop's configuration information.  i.e., where this\n",
            "# file is living. If this is not defined, Hadoop will attempt to\n",
            "# locate it based upon its execution path.\n",
            "#\n",
            "# NOTE: It is recommend that this variable not be set here but in\n",
            "# /etc/profile.d or equivalent.  Some options (such as\n",
            "# --config) may react strangely otherwise.\n",
            "#\n",
            "# export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop\n",
            "\n",
            "# The maximum amount of heap to use (Java -Xmx).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xmx setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MAX=\n",
            "\n",
            "# The minimum amount of heap to use (Java -Xms).  If no unit\n",
            "# is provided, it will be converted to MB.  Daemons will\n",
            "# prefer any Xms setting in their respective _OPT variable.\n",
            "# There is no default; the JVM will autoscale based upon machine\n",
            "# memory size.\n",
            "# export HADOOP_HEAPSIZE_MIN=\n",
            "\n",
            "# Enable extra debugging of Hadoop's JAAS binding, used to set up\n",
            "# Kerberos security.\n",
            "# export HADOOP_JAAS_DEBUG=true\n",
            "\n",
            "# Extra Java runtime options for all Hadoop commands. We don't support\n",
            "# IPv6 yet/still, so by default the preference is set to IPv4.\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true\"\n",
            "# For Kerberos debugging, an extended option set logs more information\n",
            "# export HADOOP_OPTS=\"-Djava.net.preferIPv4Stack=true -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug\"\n",
            "\n",
            "# Some parts of the shell code may do special things dependent upon\n",
            "# the operating system.  We have to set this here. See the next\n",
            "# section as to why....\n",
            "export HADOOP_OS_TYPE=${HADOOP_OS_TYPE:-$(uname -s)}\n",
            "\n",
            "# Extra Java runtime options for some Hadoop commands\n",
            "# and clients (i.e., hdfs dfs -blah).  These get appended to HADOOP_OPTS for\n",
            "# such commands.  In most cases, # this should be left empty and\n",
            "# let users supply it on the command line.\n",
            "# export HADOOP_CLIENT_OPTS=\"\"\n",
            "\n",
            "#\n",
            "# A note about classpaths.\n",
            "#\n",
            "# By default, Apache Hadoop overrides Java's CLASSPATH\n",
            "# environment variable.  It is configured such\n",
            "# that it starts out blank with new entries added after passing\n",
            "# a series of checks (file/dir exists, not already listed aka\n",
            "# de-deduplication).  During de-deduplication, wildcards and/or\n",
            "# directories are *NOT* expanded to keep it simple. Therefore,\n",
            "# if the computed classpath has two specific mentions of\n",
            "# awesome-methods-1.0.jar, only the first one added will be seen.\n",
            "# If two directories are in the classpath that both contain\n",
            "# awesome-methods-1.0.jar, then Java will pick up both versions.\n",
            "\n",
            "# An additional, custom CLASSPATH. Site-wide configs should be\n",
            "# handled via the shellprofile functionality, utilizing the\n",
            "# hadoop_add_classpath function for greater control and much\n",
            "# harder for apps/end-users to accidentally override.\n",
            "# Similarly, end users should utilize ${HOME}/.hadooprc .\n",
            "# This variable should ideally only be used as a short-cut,\n",
            "# interactive way for temporary additions on the command line.\n",
            "# export HADOOP_CLASSPATH=\"/some/cool/path/on/your/machine\"\n",
            "\n",
            "# Should HADOOP_CLASSPATH be first in the official CLASSPATH?\n",
            "# export HADOOP_USER_CLASSPATH_FIRST=\"yes\"\n",
            "\n",
            "# If HADOOP_USE_CLIENT_CLASSLOADER is set, the classpath along\n",
            "# with the main jar are handled by a separate isolated\n",
            "# client classloader when 'hadoop jar', 'yarn jar', or 'mapred job'\n",
            "# is utilized. If it is set, HADOOP_CLASSPATH and\n",
            "# HADOOP_USER_CLASSPATH_FIRST are ignored.\n",
            "# export HADOOP_USE_CLIENT_CLASSLOADER=true\n",
            "\n",
            "# HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES overrides the default definition of\n",
            "# system classes for the client classloader when HADOOP_USE_CLIENT_CLASSLOADER\n",
            "# is enabled. Names ending in '.' (period) are treated as package names, and\n",
            "# names starting with a '-' are treated as negative matches. For example,\n",
            "# export HADOOP_CLIENT_CLASSLOADER_SYSTEM_CLASSES=\"-org.apache.hadoop.UserClass,java.,javax.,org.apache.hadoop.\"\n",
            "\n",
            "# Enable optional, bundled Hadoop features\n",
            "# This is a comma delimited list.  It may NOT be overridden via .hadooprc\n",
            "# Entries may be added/removed as needed.\n",
            "# export HADOOP_OPTIONAL_TOOLS=\"hadoop-kafka,hadoop-openstack,hadoop-azure-datalake,hadoop-aliyun,hadoop-aws,hadoop-azure\"\n",
            "\n",
            "###\n",
            "# Options for remote shell connectivity\n",
            "###\n",
            "\n",
            "# There are some optional components of hadoop that allow for\n",
            "# command and control of remote hosts.  For example,\n",
            "# start-dfs.sh will attempt to bring up all NNs, DNS, etc.\n",
            "\n",
            "# Options to pass to SSH when one of the \"log into a host and\n",
            "# start/stop daemons\" scripts is executed\n",
            "# export HADOOP_SSH_OPTS=\"-o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10s\"\n",
            "\n",
            "# The built-in ssh handler will limit itself to 10 simultaneous connections.\n",
            "# For pdsh users, this sets the fanout size ( -f )\n",
            "# Change this to increase/decrease as necessary.\n",
            "# export HADOOP_SSH_PARALLEL=10\n",
            "\n",
            "# Filename which contains all of the hosts for any remote execution\n",
            "# helper scripts # such as workers.sh, start-dfs.sh, etc.\n",
            "# export HADOOP_WORKERS=\"${HADOOP_CONF_DIR}/workers\"\n",
            "\n",
            "###\n",
            "# Options for all daemons\n",
            "###\n",
            "#\n",
            "\n",
            "#\n",
            "# Many options may also be specified as Java properties.  It is\n",
            "# very common, and in many cases, desirable, to hard-set these\n",
            "# in daemon _OPTS variables.  Where applicable, the appropriate\n",
            "# Java property is also identified.  Note that many are re-used\n",
            "# or set differently in certain contexts (e.g., secure vs\n",
            "# non-secure)\n",
            "#\n",
            "\n",
            "# Where (primarily) daemon log files are stored.\n",
            "# ${HADOOP_HOME}/logs by default.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs\n",
            "\n",
            "# A string representing this instance of hadoop. $USER by default.\n",
            "# This is used in writing log and pid files, so keep that in mind!\n",
            "# Java property: hadoop.id.str\n",
            "# export HADOOP_IDENT_STRING=$USER\n",
            "\n",
            "# How many seconds to pause after stopping a daemon\n",
            "# export HADOOP_STOP_TIMEOUT=5\n",
            "\n",
            "# Where pid files are stored.  /tmp by default.\n",
            "# export HADOOP_PID_DIR=/tmp\n",
            "\n",
            "# Default log4j setting for interactive commands\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_ROOT_LOGGER=INFO,console\n",
            "\n",
            "# Default log4j setting for daemons spawned explicitly by\n",
            "# --daemon option of hadoop, hdfs, mapred and yarn command.\n",
            "# Java property: hadoop.root.logger\n",
            "# export HADOOP_DAEMON_ROOT_LOGGER=INFO,RFA\n",
            "\n",
            "# Default log level and output location for security-related messages.\n",
            "# You will almost certainly want to change this on a per-daemon basis via\n",
            "# the Java property (i.e., -Dhadoop.security.logger=foo). (Note that the\n",
            "# defaults for the NN and 2NN override this by default.)\n",
            "# Java property: hadoop.security.logger\n",
            "# export HADOOP_SECURITY_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Default process priority level\n",
            "# Note that sub-processes will also run at this level!\n",
            "# export HADOOP_NICENESS=0\n",
            "\n",
            "# Default name for the service level authorization file\n",
            "# Java property: hadoop.policy.file\n",
            "# export HADOOP_POLICYFILE=\"hadoop-policy.xml\"\n",
            "\n",
            "#\n",
            "# NOTE: this is not used by default!  <-----\n",
            "# You can define variables right here and then re-use them later on.\n",
            "# For example, it is common to use the same garbage collection settings\n",
            "# for all the daemons.  So one could define:\n",
            "#\n",
            "# export HADOOP_GC_SETTINGS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps\"\n",
            "#\n",
            "# .. and then use it as per the b option under the namenode.\n",
            "\n",
            "###\n",
            "# Secure/privileged execution\n",
            "###\n",
            "\n",
            "#\n",
            "# Out of the box, Hadoop uses jsvc from Apache Commons to launch daemons\n",
            "# on privileged ports.  This functionality can be replaced by providing\n",
            "# custom functions.  See hadoop-functions.sh for more information.\n",
            "#\n",
            "\n",
            "# The jsvc implementation to use. Jsvc is required to run secure datanodes\n",
            "# that bind to privileged ports to provide authentication of data transfer\n",
            "# protocol.  Jsvc is not required if SASL is configured for authentication of\n",
            "# data transfer protocol using non-privileged ports.\n",
            "# export JSVC_HOME=/usr/bin\n",
            "\n",
            "#\n",
            "# This directory contains pids for secure and privileged processes.\n",
            "#export HADOOP_SECURE_PID_DIR=${HADOOP_PID_DIR}\n",
            "\n",
            "#\n",
            "# This directory contains the logs for secure and privileged processes.\n",
            "# Java property: hadoop.log.dir\n",
            "# export HADOOP_SECURE_LOG=${HADOOP_LOG_DIR}\n",
            "\n",
            "#\n",
            "# When running a secure daemon, the default value of HADOOP_IDENT_STRING\n",
            "# ends up being a bit bogus.  Therefore, by default, the code will\n",
            "# replace HADOOP_IDENT_STRING with HADOOP_xx_SECURE_USER.  If one wants\n",
            "# to keep HADOOP_IDENT_STRING untouched, then uncomment this line.\n",
            "# export HADOOP_SECURE_IDENT_PRESERVE=\"true\"\n",
            "\n",
            "###\n",
            "# NameNode specific parameters\n",
            "###\n",
            "\n",
            "# Default log level and output location for file system related change\n",
            "# messages. For non-namenode daemons, the Java property must be set in\n",
            "# the appropriate _OPTS if one wants something other than INFO,NullAppender\n",
            "# Java property: hdfs.audit.logger\n",
            "# export HDFS_AUDIT_LOGGER=INFO,NullAppender\n",
            "\n",
            "# Specify the JVM options to be used when starting the NameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# a) Set JMX options\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.port=1026\"\n",
            "#\n",
            "# b) Set garbage collection logs\n",
            "# export HDFS_NAMENODE_OPTS=\"${HADOOP_GC_SETTINGS} -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "#\n",
            "# c) ... or set them directly\n",
            "# export HDFS_NAMENODE_OPTS=\"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -Xloggc:${HADOOP_LOG_DIR}/gc-rm.log-$(date +'%Y%m%d%H%M')\"\n",
            "\n",
            "# this is the default:\n",
            "# export HDFS_NAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# SecondaryNameNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the SecondaryNameNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_SECONDARYNAMENODE_OPTS=\"-Dhadoop.security.logger=INFO,RFAS\"\n",
            "\n",
            "###\n",
            "# DataNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the DataNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# This is the default:\n",
            "# export HDFS_DATANODE_OPTS=\"-Dhadoop.security.logger=ERROR,RFAS\"\n",
            "\n",
            "# On secure datanodes, user to run the datanode as after dropping privileges.\n",
            "# This **MUST** be uncommented to enable secure HDFS if using privileged ports\n",
            "# to provide authentication of data transfer protocol.  This **MUST NOT** be\n",
            "# defined if SASL is configured for authentication of data transfer protocol\n",
            "# using non-privileged ports.\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_DATANODE_SECURE_USER=hdfs\n",
            "\n",
            "# Supplemental options for secure datanodes\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_DATANODE_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "###\n",
            "# NFS3 Gateway specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the NFS3 Gateway.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_NFS3_OPTS=\"\"\n",
            "\n",
            "# Specify the JVM options to be used when starting the Hadoop portmapper.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_PORTMAP_OPTS=\"-Xmx512m\"\n",
            "\n",
            "# Supplemental options for priviliged gateways\n",
            "# By default, Hadoop uses jsvc which needs to know to launch a\n",
            "# server jvm.\n",
            "# export HDFS_NFS3_SECURE_EXTRA_OPTS=\"-jvm server\"\n",
            "\n",
            "# On privileged gateways, user to run the gateway as after dropping privileges\n",
            "# This will replace the hadoop.id.str Java property in secure mode.\n",
            "# export HDFS_NFS3_SECURE_USER=nfsserver\n",
            "\n",
            "###\n",
            "# ZKFailoverController specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the ZKFailoverController.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_ZKFC_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# QuorumJournalNode specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the QuorumJournalNode.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_JOURNALNODE_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Balancer specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Balancer.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_BALANCER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS Mover specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Mover.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_MOVER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Router-based HDFS Federation specific parameters\n",
            "# Specify the JVM options to be used when starting the RBF Routers.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_DFSROUTER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# HDFS StorageContainerManager specific parameters\n",
            "###\n",
            "# Specify the JVM options to be used when starting the HDFS Storage Container Manager.\n",
            "# These options will be appended to the options specified as HADOOP_OPTS\n",
            "# and therefore may override any similar flags set in HADOOP_OPTS\n",
            "#\n",
            "# export HDFS_STORAGECONTAINERMANAGER_OPTS=\"\"\n",
            "\n",
            "###\n",
            "# Advanced Users Only!\n",
            "###\n",
            "\n",
            "#\n",
            "# When building Hadoop, one can add the class paths to the commands\n",
            "# via this special env var:\n",
            "# export HADOOP_ENABLE_BUILD_PATHS=\"true\"\n",
            "\n",
            "#\n",
            "# To prevent accidents, shell commands be (superficially) locked\n",
            "# to only allow certain users to execute certain subcommands.\n",
            "# It uses the format of (command)_(subcommand)_USER.\n",
            "#\n",
            "# For example, to limit who can execute the namenode command,\n",
            "# export HDFS_NAMENODE_USER=hdfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only required enviroment variable is JAVA_HOME. All the others are optional.\n",
        "\n",
        "To specify the JAVA_HOME variable in hadoop-env.sh we need to uncomment the export line and update it with the actual directory.\n",
        "\n",
        "In this case it should look like this:\n",
        "\n",
        "export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64"
      ],
      "metadata": {
        "id": "C41AnZkIV9nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding JAVA_HOME directory to hadoop-env.sh file\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "cgwpEKXNT0eY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Creating environment variables\n",
        "#Creating Hadoop home variable\n",
        "\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += f'{os.environ[\"JAVA_HOME\"]}/bin:{os.environ[\"JRE_HOME\"]}/bin:{os.environ[\"HADOOP_HOME\"]}/sbin'"
      ],
      "metadata": {
        "id": "2lIy0zPdNcxR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring XML files\n",
        "\n",
        "The majority of Hadoop setting are contained in XML configuration files. These files are also known as resources.\n",
        "\n",
        "They have the following structure:\n",
        "\n",
        "<configuration>\n",
        "...\n",
        "  <property>\n",
        "    <name>...</name>\n",
        "    <value>...</value>\n",
        "    <description>...</description>\n",
        "  </property>\n",
        "...\n",
        "</configuration>\n",
        "The XLM file can contained any number of the property elements. Each property element defines a specific configuration name-value pair.\n",
        "\n",
        "Hadoop configuration is driven by two distict types of XLM configuration files:\n",
        "\n",
        "Default (read-only): core-default.xml, hdfs-default.xml, mapred-default.xml, yarn-default.xml. These files should never be modified.\n",
        "Site specific configuration files: core-site.xml, hdfs-site.xml, mapred-site.xml, yarn-site.xml. These files are loaded from class path and their values are used to overwrite the corresponding values of the properties in the matching default configuration files."
      ],
      "metadata": {
        "id": "1lQehM_OWpQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3/etc/hadoop xml files\n",
        "!ls $HADOOP_HOME/etc/hadoop/*.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af2q-JuwUL6p",
        "outputId": "23e22573-f69a-4aa8-fe17-bb1cea5dd5e7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.2.3/etc/hadoop/capacity-scheduler.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/core-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hadoop-policy.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/hdfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/httpfs-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-acls.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/kms-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/mapred-site.xml\n",
            "/usr/local/hadoop-3.2.3/etc/hadoop/yarn-site.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Hadoop in Pseudo-distributed mode\n",
        "In Pseudo-distributed mode all the distributed components of Hadoop come into play. That is, all the Hadoop deamons that are responsible for distributed storage and distributed processing will run on the same machine.\n",
        "\n",
        "Master deamons:\n",
        "\n",
        "NameNode\n",
        "\n",
        "Resource Manager\n",
        "\n",
        "Standby NameNode\n",
        "\n",
        "Slave deamons:\n",
        "\n",
        "DataNode\n",
        "\n",
        "Node Manager\n",
        "\n",
        "### Configuring XML files\n",
        "\n",
        "As mentioned, by setting the properties in the site xml configuration files, we overwrite the corresponding properties in the default xml configuration files and, this way, we tell Hadoop which machines are in the cluster and where and how we want to run the Hadoop daemons\n",
        "\n",
        "The specific content that these files need to have to make Hadoop run in Pseudo-distributed mode can be found in the documentation of the release on the official website. For Hadoop 3.2.3 the website is:\n",
        "\n",
        "https://hadoop.apache.org/docs/r3.2.3/hadoop-project-dist/hadoop-common/SingleCluster.html\n",
        "\n",
        "Each component in Hadoop is configured using an xml file\n",
        "\n",
        "core-site.xml: common properties\n",
        "\n",
        "hdfs-site.xml: HDFS properties\n",
        "\n",
        "mapred-site.xml: MapReduce properties\n",
        "\n",
        "yarn-site.xml: YARN properties\n",
        "\n",
        "By configuring these xml files accordingly Hadoop can be run in one of the three modes."
      ],
      "metadata": {
        "id": "2ITbTQitWw2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml file\n",
        "# !cp /content/sample_data/core-site.xml $HADOOP_HOME/etc/hadoop/\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GntkmvjzUNj7",
        "outputId": "dd8a9f37-377b-431b-bb2d-253409e05fd7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dowloading text example to use as input\n",
        "# !wget -q https://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/101/101.txt"
      ],
      "metadata": {
        "id": "y71dZGccURYd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding required property to core-site.xlm file\n",
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "  <property>\n",
        "          <name>fs.defaultFS</name>\n",
        "          <value>hdfs://localhost:9000</value>\n",
        "          <description>Where HDFS NameNode can be found on the network</description>\n",
        "  </property>\n",
        "</configuration>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK_Ozh8RVza-",
        "outputId": "6c874abe-ba81-4409-bc50-accc1e9b5d9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 26: warning: here-document at line 1 delimited by end-of-file (wanted `EOF')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Content of core-site.xml after the editing\n",
        "!cat $HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PkowknHV25K",
        "outputId": "df068cdf-b9e5-4c9f-e38c-46b1e479d302"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "  <property>\n",
            "          <name>fs.defaultFS</name>\n",
            "          <value>hdfs://localhost:9000</value>\n",
            "          <description>Where HDFS NameNode can be found on the network</description>\n",
            "  </property>\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
        "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>dfs.replication</name>\n",
        "    <value>1</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ],
      "metadata": {
        "id": "iRlijZzZ5_IQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBJ98XdF8e8z",
        "outputId": "fed9a6f7-7e34-4370-d293-591b6b459a90"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "    <name>dfs.replication</name>\n",
            "    <value>1</value>\n",
            "  </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "\n",
        "<!-- Put site-specific property overrides in this file. -->\n",
        "\n",
        "<configuration>\n",
        "<property>\n",
        "    <name>mapreduce.framework.name</name>\n",
        "    <value>yarn</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>mapreduce.application.classpath</name>\n",
        "    <value>$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*</value>\n",
        "  </property>\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ],
      "metadata": {
        "id": "oI7EXvcy8x5Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2m3lhhv85H3",
        "outputId": "0f90a7fb-c62b-4dbf-da9c-d27909a97032"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "\n",
            "<!-- Put site-specific property overrides in this file. -->\n",
            "\n",
            "<configuration>\n",
            "<property>\n",
            "    <name>mapreduce.framework.name</name>\n",
            "    <value>yarn</value>\n",
            "  </property>\n",
            "  <property>\n",
            "    <name>mapreduce.application.classpath</name>\n",
            "    <value>/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/*:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/*</value>\n",
            "  </property>\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat <<EOF > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
        "<?xml version=\"1.0\"?>\n",
        "<!--\n",
        "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "  you may not use this file except in compliance with the License.\n",
        "  You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "  Unless required by applicable law or agreed to in writing, software\n",
        "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "  See the License for the specific language governing permissions and\n",
        "  limitations under the License. See accompanying LICENSE file.\n",
        "-->\n",
        "<configuration>\n",
        "<property>\n",
        "    <description>The hostname of the RM.</description>\n",
        "    <name>yarn.resourcemanager.hostname</name>\n",
        "    <value>localhost</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.aux-services</name>\n",
        "    <value>mapreduce_shuffle</value>\n",
        "  </property>\n",
        "  <property>\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
        "  </property>\n",
        "\n",
        "<!-- Site specific YARN configuration properties -->\n",
        "\n",
        "</configuration>\n",
        "EOF"
      ],
      "metadata": {
        "id": "VXqsVh1M9Pxu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat $HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVR7LFSJ9pug",
        "outputId": "fd06d1e8-5795-40f0-a24f-a355476e6fde"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<?xml version=\"1.0\"?>\n",
            "<!--\n",
            "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "  you may not use this file except in compliance with the License.\n",
            "  You may obtain a copy of the License at\n",
            "\n",
            "    http://www.apache.org/licenses/LICENSE-2.0\n",
            "\n",
            "  Unless required by applicable law or agreed to in writing, software\n",
            "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "  See the License for the specific language governing permissions and\n",
            "  limitations under the License. See accompanying LICENSE file.\n",
            "-->\n",
            "<configuration>\n",
            "<property>\n",
            "    <description>The hostname of the RM.</description>\n",
            "    <name>yarn.resourcemanager.hostname</name>\n",
            "    <value>localhost</value>\n",
            "  </property>\n",
            "  <property>\n",
            "    <name>yarn.nodemanager.aux-services</name>\n",
            "    <value>mapreduce_shuffle</value>\n",
            "  </property>\n",
            "  <property>\n",
            "    <name>yarn.nodemanager.env-whitelist</name>\n",
            "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\n",
            "  </property>\n",
            "\n",
            "<!-- Site specific YARN configuration properties -->\n",
            "\n",
            "</configuration>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatting the HDFS Filesystem\n",
        "\n",
        "Before HDFS can be used for the first time the file system must be formatted. The formatting process creates an empty file system by creating the storage directories and the initial versions of the NameNodes"
      ],
      "metadata": {
        "id": "WeDr2ydyXYFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmrjTfnQ9rzL",
        "outputId": "fdf27e53-8391-49c0-f534-fd56f3a3c377"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2024-04-22 17:36:43,731 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 3db6ed18c7d2/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_402\n",
            "************************************************************/\n",
            "2024-04-22 17:36:43,758 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2024-04-22 17:36:43,995 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-e88a48f4-b082-46b7-8a6f-0a761578b3f9\n",
            "2024-04-22 17:36:45,262 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2024-04-22 17:36:45,349 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2024-04-22 17:36:45,358 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2024-04-22 17:36:45,359 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2024-04-22 17:36:45,374 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2024-04-22 17:36:45,374 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2024-04-22 17:36:45,381 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2024-04-22 17:36:45,381 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2024-04-22 17:36:45,466 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2024-04-22 17:36:45,482 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2024-04-22 17:36:45,483 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2024-04-22 17:36:45,490 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2024-04-22 17:36:45,491 INFO blockmanagement.BlockManager: The block deletion will start around 2024 Apr 22 17:36:45\n",
            "2024-04-22 17:36:45,493 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2024-04-22 17:36:45,493 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-22 17:36:45,495 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2024-04-22 17:36:45,495 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2024-04-22 17:36:45,538 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2024-04-22 17:36:45,538 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2024-04-22 17:36:45,579 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2024-04-22 17:36:45,579 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2024-04-22 17:36:45,579 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2024-04-22 17:36:45,580 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2024-04-22 17:36:45,581 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2024-04-22 17:36:45,621 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2024-04-22 17:36:45,621 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2024-04-22 17:36:45,621 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2024-04-22 17:36:45,621 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2024-04-22 17:36:45,643 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2024-04-22 17:36:45,643 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-22 17:36:45,644 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2024-04-22 17:36:45,644 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2024-04-22 17:36:45,647 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2024-04-22 17:36:45,647 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2024-04-22 17:36:45,647 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2024-04-22 17:36:45,648 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2024-04-22 17:36:45,656 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2024-04-22 17:36:45,665 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2024-04-22 17:36:45,672 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2024-04-22 17:36:45,672 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-22 17:36:45,672 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2024-04-22 17:36:45,672 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2024-04-22 17:36:45,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2024-04-22 17:36:45,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2024-04-22 17:36:45,684 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2024-04-22 17:36:45,696 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2024-04-22 17:36:45,697 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2024-04-22 17:36:45,699 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2024-04-22 17:36:45,699 INFO util.GSet: VM type       = 64-bit\n",
            "2024-04-22 17:36:45,700 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2024-04-22 17:36:45,700 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2024-04-22 17:36:45,764 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1786506445-172.28.0.12-1713807405742\n",
            "2024-04-22 17:36:45,820 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2024-04-22 17:36:45,930 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2024-04-22 17:36:46,099 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2024-04-22 17:36:46,127 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2024-04-22 17:36:46,216 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2024-04-22 17:36:46,217 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2024-04-22 17:36:46,229 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2024-04-22 17:36:46,229 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 3db6ed18c7d2/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring Hadoop scripts available in sbin directory\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWxbA7Ah-ML5",
        "outputId": "3d1cd4b5-3368-4c2d-c872-21eeda3b1cff"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t refresh-namenodes.sh  start-yarn.cmd\t stop-secure-dns.sh\n",
            "FederationStateStore\t start-all.cmd\t       start-yarn.sh\t stop-yarn.cmd\n",
            "hadoop-daemon.sh\t start-all.sh\t       stop-all.cmd\t stop-yarn.sh\n",
            "hadoop-daemons.sh\t start-balancer.sh     stop-all.sh\t workers.sh\n",
            "httpfs.sh\t\t start-dfs.cmd\t       stop-balancer.sh  yarn-daemon.sh\n",
            "kms.sh\t\t\t start-dfs.sh\t       stop-dfs.cmd\t yarn-daemons.sh\n",
            "mr-jobhistory-daemon.sh  start-secure-dns.sh   stop-dfs.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating other necessary enviroment variables before starting nodes\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "Cw-pv2oM-Rnu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching hdfs deamons\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh0QqrSw-U8A",
        "outputId": "913c87ed-7278-48ec-e951-c982bb46eacf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [3db6ed18c7d2]\n",
            "3db6ed18c7d2: Warning: Permanently added '3db6ed18c7d2' (ED25519) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf1kbIOY_PXl",
        "outputId": "e9873f94-91ec-428c-b95f-45d0b2655323"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2145 SecondaryNameNode\n",
            "1845 NameNode\n",
            "1948 DataNode\n",
            "2413 Jps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Launching yarn deamons\n",
        "#nohup causes a process to ignore a SIGHUP signal\n",
        "!nohup $HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqE22Z1V_nyL",
        "outputId": "74b3553c-c478-4c27-86e2-0c40c27c977e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: ignoring input and appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Listing the running deamons\n",
        "!jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rco2XCQ_pMa",
        "outputId": "e1f91dea-a76c-45ac-d6a3-63ebe231585f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2145 SecondaryNameNode\n",
            "2740 Jps\n",
            "1845 NameNode\n",
            "2645 NodeManager\n",
            "2534 ResourceManager\n",
            "1948 DataNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring Hadoop cluster with hadoop admin commands"
      ],
      "metadata": {
        "id": "srV27xSGXu5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Report the basic file system information and statistics\n",
        "!$HADOOP_HOME/bin/hdfs dfsadmin -report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUxnt00v_wqC",
        "outputId": "cf128ed1-33d1-44ad-bc97-1de711811792"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "Present Capacity: 84939431936 (79.11 GB)\n",
            "DFS Remaining: 84939407360 (79.11 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "DFS Used%: 0.00%\n",
            "Replicated Blocks:\n",
            "\tUnder replicated blocks: 0\n",
            "\tBlocks with corrupt replicas: 0\n",
            "\tMissing blocks: 0\n",
            "\tMissing blocks (with replication factor 1): 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "Erasure Coded Block Groups: \n",
            "\tLow redundancy block groups: 0\n",
            "\tBlock groups with corrupt internal blocks: 0\n",
            "\tMissing block groups: 0\n",
            "\tLow redundancy blocks with highest priority to recover: 0\n",
            "\tPending deletion blocks: 0\n",
            "\n",
            "-------------------------------------------------\n",
            "Live datanodes (1):\n",
            "\n",
            "Name: 127.0.0.1:9866 (localhost)\n",
            "Hostname: 3db6ed18c7d2\n",
            "Decommission Status : Normal\n",
            "Configured Capacity: 115658190848 (107.72 GB)\n",
            "DFS Used: 24576 (24 KB)\n",
            "Non DFS Used: 30701981696 (28.59 GB)\n",
            "DFS Remaining: 84939407360 (79.11 GB)\n",
            "DFS Used%: 0.00%\n",
            "DFS Remaining%: 73.44%\n",
            "Configured Cache Capacity: 0 (0 B)\n",
            "Cache Used: 0 (0 B)\n",
            "Cache Remaining: 0 (0 B)\n",
            "Cache Used%: 100.00%\n",
            "Cache Remaining%: 0.00%\n",
            "Xceivers: 1\n",
            "Last contact: Mon Apr 22 17:38:01 UTC 2024\n",
            "Last Block Report: Mon Apr 22 17:37:11 UTC 2024\n",
            "Num of Blocks: 0\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS (Hadoop Distributed File System)\n",
        "\n",
        "1. Setup HDFS: First, make sure you have Hadoop installed and running. You can follow online tutorials or use a Hadoop distribution like Apache Hadoop or Cloudera.\n",
        "\n",
        "2. Create a directory: Use the HDFS command-line interface (CLI) to create a directory. For example:"
      ],
      "metadata": {
        "id": "qkINc9skYBld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating directory in HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /user/input"
      ],
      "metadata": {
        "id": "kL5_9fK5AHIZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Copy a file to HDFS: You can copy a file from your local filesystem to HDFS using the put command. For example:"
      ],
      "metadata": {
        "id": "GnuEQpatrQPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put localfile.txt /user/input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmCUVTiRrSmK",
        "outputId": "2f6225dc-d877-4b3b-e48b-8336ecf3e7fc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "put: `localfile.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List files in HDFS: To see the files in a directory in HDFS, you can use the ls command. For example:"
      ],
      "metadata": {
        "id": "7HE4FpRDrbLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /user/input"
      ],
      "metadata": {
        "id": "W6BDFFkDrcsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Read a file from HDFS: You can read the contents of a file in HDFS using the cat command. For example:"
      ],
      "metadata": {
        "id": "U_KHiCBPrj5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /user/input/localfile.txt"
      ],
      "metadata": {
        "id": "Vr-8rqeMrlQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Delete a file: You can delete a file from HDFS using the rm command. For example:"
      ],
      "metadata": {
        "id": "ZJjuv_N5ruM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -rm /user/input/localfile.txt"
      ],
      "metadata": {
        "id": "5xeFf4lsrxYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real Example\n",
        "1. Scenario: Let's say you have a dataset containing logs from multiple servers, and you want to store and analyze these logs using Hadoop ecosystem tools.\n",
        "\n",
        "2. Setup HDFS: Ensure that your Hadoop cluster is up and running, including the HDFS component.\n",
        "Create directories in HDFS: Create directories in HDFS to organize your data. For example:"
      ],
      "metadata": {
        "id": "UVEa1-wnsAV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /logs\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /logs/server1\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /logs/server2"
      ],
      "metadata": {
        "id": "ewlMtrkxsI09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Copy log files to HDFS: Use the put command to copy your log files to HDFS. For example:"
      ],
      "metadata": {
        "id": "UIfx6GrSsRsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -put server1_logs.txt /logs/server1\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put server2_logs.txt /logs/server2"
      ],
      "metadata": {
        "id": "yMgJ4abVsS7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List files in HDFS: Verify that the log files are successfully copied to HDFS:"
      ],
      "metadata": {
        "id": "o-MXnCIOsZGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -ls /logs/server1\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /logs/server2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96-2TFsVsanh",
        "outputId": "397d0bda-4911-4bab-b6cc-6637d24561fe"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: `/logs/server1': No such file or directory\n",
            "ls: `/logs/server2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Read files from HDFS: You can read the contents of the log files using the cat command:"
      ],
      "metadata": {
        "id": "jQ-Rw3S7shAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -cat /logs/server1/server1_logs.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /logs/server2/server2_logs.txt"
      ],
      "metadata": {
        "id": "4rb5uF2UskHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir -p /logs/server1\n",
        "!$HADOOP_HOME/bin/hdfs dfs -touchs /logs/server1/server1_logs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_ZKhna5rh8",
        "outputId": "b2546e21-713d-4060-b526-a62aada31da8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: /bin/hdfs: No such file or directory\n",
            "/bin/bash: line 1: /bin/hdfs: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Perform data analysis: You can now use Hadoop ecosystem tools like MapReduce, Apache Spark, or Apache Hive to analyze the log data stored in HDFS. For example, you could write a MapReduce job to count the occurrences of specific events in the logs or use Spark SQL to perform complex analytics.\n",
        "\n",
        "7. Cleanup: Once you're done with the analysis, you can delete the log files from HDFS:"
      ],
      "metadata": {
        "id": "3-n6e5M_tAFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow\n",
        "!pip install hdfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFhVb0fJtS2e",
        "outputId": "4eac2676-b6de-47da-c524-611bbc7afbbd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n",
            "Collecting hdfs\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from hdfs)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (2.31.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hdfs) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->hdfs) (2024.2.2)\n",
            "Building wheels for collected packages: hdfs, docopt\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34324 sha256=bb89a99696e3c1f741f869e945f75b60829c2bf89600bfcc431a870960a8b322\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=1e793d044704a474c51b327cc1c34f496fd95cb629731ddb0e64e958e3b58c78\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built hdfs docopt\n",
            "Installing collected packages: docopt, hdfs\n",
            "Successfully installed docopt-0.6.2 hdfs-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['CLASSPATH'] = f'{os.environ[\"HADOOP_HOME\"]}/bin/hdfs classpath --glob'\n",
        "os.environ['ARROW_LIBHDFS_DIR'] = f'{os.environ[\"HADOOP_HOME\"]}/lib/native'"
      ],
      "metadata": {
        "id": "fFxSc8DnuUaA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install telnet\n",
        "!telnet localhost 9000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2PGa7zK43iJ",
        "outputId": "28403914-88a9-430e-96bc-d897baf63673"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  netbase\n",
            "The following NEW packages will be installed:\n",
            "  netbase telnet\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 79.9 kB of archives.\n",
            "After this operation, 200 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 telnet amd64 0.17-44build1 [66.9 kB]\n",
            "Fetched 79.9 kB in 0s (188 kB/s)\n",
            "Selecting previously unselected package netbase.\n",
            "(Reading database ... 124990 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package telnet.\n",
            "Preparing to unpack .../telnet_0.17-44build1_amd64.deb ...\n",
            "Unpacking telnet (0.17-44build1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up telnet (0.17-44build1) ...\n",
            "update-alternatives: using /usr/bin/telnet.netkit to provide /usr/bin/telnet (telnet) in auto mode\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Trying 127.0.0.1...\n",
            "Connected to localhost.\n",
            "Escape character is '^]'.\n",
            "Connection closed by foreign host.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow\n",
        "\n",
        "# Specify the hostname and port of the HDFS NameNode\n",
        "hdfs_host = 'localhost'\n",
        "hdfs_port = 9000\n",
        "\n",
        "# Connect to HDFS\n",
        "# fs = pyarrow.hdfs.connect(host=hdfs_host, port=hdfs_port)\n",
        "hdfs = pyarrow.HadoopFileSystem(hdfs_host, hdfs_port, user='root')\n",
        "hdfs = pyarrow.hdfs.connect()\n",
        "\n",
        "# # Specify the path to the file in HDFS\n",
        "file_path = '/logs/server1/server1_logs.txt'\n",
        "\n",
        "# # Read the file from HDFS\n",
        "with hdfs.open(file_path, 'rb') as f:\n",
        "    # Perform analysis, for example, counting lines\n",
        "    line_count = sum(1 for line in f)\n",
        "\n",
        "# print(\"Number of lines in the file:\", line_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "rxLnjo5ntHql",
        "outputId": "e91c42f8-80e8-4084-8fea-2c23a17d0fb8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-1d5662ec6ddc>:9: FutureWarning: pyarrow.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
            "  hdfs = pyarrow.HadoopFileSystem(hdfs_host, hdfs_port, user='root')\n",
            "<ipython-input-47-1d5662ec6ddc>:9: FutureWarning: pyarrow.hdfs.HadoopFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
            "  hdfs = pyarrow.HadoopFileSystem(hdfs_host, hdfs_port, user='root')\n",
            "<ipython-input-47-1d5662ec6ddc>:10: FutureWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
            "  hdfs = pyarrow.hdfs.connect()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Opening HDFS file '/logs/server1/server1_logs.txt' failed. Detail: [errno 2] No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1d5662ec6ddc>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# # Read the file from HDFS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mhdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Perform analysis, for example, counting lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mline_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_hdfsio.pyx\u001b[0m in \u001b[0;36mpyarrow._hdfsio.HadoopFileSystem.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Opening HDFS file '/logs/server1/server1_logs.txt' failed. Detail: [errno 2] No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!$HADOOP_HOME/bin/hdfs dfs -rm /logs/server1/server1_logs.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -rm /logs/server2/server2_logs.txt"
      ],
      "metadata": {
        "id": "T_KpYB2PtC3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}